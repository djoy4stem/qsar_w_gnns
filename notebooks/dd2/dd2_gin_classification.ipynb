{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IPython extensions to reload modules before executing user code.\n",
    "# Autorelad is an IPython extension to reload modules before executing user code.\n",
    "%load_ext autoreload\n",
    "\n",
    "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/data\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "ROOT_DIR = os.sep.join(os.path.abspath('.').split(os.sep)[:-2])\n",
    "sys.path.insert(0, ROOT_DIR)\n",
    "DATASET_DIR = \"{}/data\".format(ROOT_DIR)\n",
    "print(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Any, Tuple\n",
    "from datetime import datetime\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import BCELoss, LeakyReLU, ReLU\n",
    "from torch.optim import lr_scheduler,Adagrad, Adadelta, Adam, AdamW, SGD\n",
    "\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Draw, AllChem, PandasTools, MolFromSequence, MolToSmiles, MolToInchiKey, Descriptors, GraphDescriptors\n",
    "from rdkit.Chem import rdMolDescriptors as rdmdesc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from lib import gnn_utils, utilities, datasets, splitters, featurizers, training_utils, graph_nns\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score, balanced_accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams.update({'font.size': 24})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set seeds\n",
    "seed = 123\n",
    "utilities.set_seeds(seed=seed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: cursive; font-size=14px\">\n",
    "In the following, we will train regression models to predict the percentage of inhibition of the <em>P. falciparum's</em> DD2 growth.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: cursive\">Data Preparation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family: cursive\">Dataset Cleaning</h4>\n",
    "<p style=\"font-family: cursive; font-size=14px\">\n",
    "The dataset was obtained from the PubChem Biossay AID 2302. It was used as example in the paper by [Chakravarti and Mani Alla (2019); Frontiers in Artificial Intelligence, 2, DOI: 10.3389/frai.2019.00017](https://www.frontiersin.org/articles/10.3389/frai.2019.00017/full). </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd2_data_df   = pd.read_csv(f\"{DATASET_DIR}/dd2_inhibition_pubchem_aid_2302.csv\")\n",
    "# smiles_column = \"PUBCHEM_EXT_DATASOURCE_SMILES\"\n",
    "# # target_column = \"PCT_INHIB_DD2\"\n",
    "# target_column = \"DD2_ACTIVITY_CLS\"\n",
    "# mol_column    = 'RMol'\n",
    "\n",
    "# print(f\"Number of molecules with a missing SMILES string: {dd2_data_df[smiles_column].isna().sum()}/{dd2_data_df.shape[0]}\")\n",
    "\n",
    "# # Removing rows with a missing SMILES string\n",
    "# dd2_data_df.dropna(subset=[smiles_column], axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# # Remove rows where is not defined\n",
    "# dd2_data_df.dropna(subset=['PUBCHEM_ACTIVITY_OUTCOME'], axis=0, inplace=True)\n",
    "\n",
    "# # Add a column with 0/1 activity values\n",
    "# dd2_data_df[target_column] = dd2_data_df['PUBCHEM_ACTIVITY_OUTCOME'].apply(lambda x: 1 if x =='Active' else 0)\n",
    "\n",
    "\n",
    "# # Adding molecule (Mol) objects to the dataframe\n",
    "# PandasTools.AddMoleculeColumnToFrame(dd2_data_df, smiles_column, mol_column)\n",
    "\n",
    "# dd2_data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family: cursive; font-size=16px\">\n",
    "Beside removing rows with an empty SMILES, we will proceed with the following steps:\n",
    "<ol>\n",
    "    <li>Select the largest fragment for each molecule, which will also remove slats, among others. We will use the SMILES and inchikeys of the fragment.</li>\n",
    "    <li>Sanitize the retained molecule</li>\n",
    "    <li>Fix the target values. Negative values are set to 0 and values greater than 100 are set to 100</li>\n",
    "    <li>Remove duplicates, using the inchikey as primary key. For each group of duplicates: </li>\n",
    "        <ol>\n",
    "            <li>If the min and max values are apart by 20 or more, omit and proceed to the next group</li>\n",
    "            <li>Else, retain the first occurrence but replace the target value by an average</li>\n",
    "            <li>It is worth mentioning that duplicate removal is usually a more involved process, where outliers can be detected and removed using a statisical approach, before averaging the taarget value.</li>\n",
    "        </ol>\n",
    "</ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd2_data_df['largest_frag']        = dd2_data_df[mol_column].apply(utilities.get_largest_fragment_from_mol)\n",
    "\n",
    "# dd2_data_df['largest_frag_smiles'] = dd2_data_df['largest_frag'].apply(lambda mol: MolToSmiles(utilities.sanitize_molecule(mol, add_explicit_h=True)))\n",
    "\n",
    "# dd2_data_df.dropna(subset=['largest_frag_smiles'], axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# dd2_data_df['largest_frag_ikey'] = dd2_data_df['largest_frag'].apply(MolToInchiKey)\n",
    "# duplicates = dd2_data_df[dd2_data_df.duplicated(subset=['largest_frag_ikey'], keep=False)].sort_values(by='largest_frag_ikey')\n",
    "\n",
    "# print(f\"Number of molecules with a missing largest fragment SMILES string: {dd2_data_df['largest_frag_smiles'].isna().sum()}/{dd2_data_df.shape[0]}\")\n",
    "\n",
    "# print(f\"Number of unique inchikeys: {dd2_data_df['largest_frag_ikey'].unique().size}/{dd2_data_df.shape[0]}\")\n",
    "# print(f\"Number of duplicates inchikeys: {duplicates.shape[0]} - e.g.: {duplicates.index[:2]}\")\n",
    "# print(f\"Number of molecules with a negative target value: {dd2_data_df[dd2_data_df[target_column]<0].shape[0]}/{dd2_data_df.shape[0]} - e.g.:{dd2_data_df[dd2_data_df[target_column]<0].index[:2]}\")\n",
    "# print(f\"Number of molecules with target value>100: {dd2_data_df[dd2_data_df[target_column]>100].shape[0]}/{dd2_data_df.shape[0]} - e.g.:{dd2_data_df[dd2_data_df[target_column]>100].index[:2]}\")\n",
    "# dd2_data_df[['PUBCHEM_CID', 'largest_frag_ikey', target_column]].loc[[8,9,272, 458, 11041, 11042],:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd2_data_df['lf_num_frags'] = dd2_data_df['largest_frag'].apply(lambda x: len(Chem.GetMolFrags(x)))\n",
    "# print(dd2_data_df[dd2_data_df['lf_num_frags']>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\" Unique target values = {dd2_data_df[target_column].unique()}\")\n",
    "\n",
    "# dd2_data_df['PUBCHEM_CID'] = dd2_data_df['PUBCHEM_CID'].astype(int)\n",
    "\n",
    "# dd2_data_df[dd2_data_df.columns.difference([mol_column, 'largest_frag', 'lf_num_frags'])].to_csv(f\"{DATASET_DIR}/dd2_inhibition_pubchem_aid_2302_cleaned.csv\")\n",
    "# dd2_data_df[['PUBCHEM_CID', 'largest_frag_smiles']].to_csv(f\"{DATASET_DIR}/dd2_inhibition_pubchem_aid_2302_largest_frag_cleaned.csv\")\n",
    "\n",
    "\n",
    "# # dd2_data_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: cursive; font-size=16px\">As you can see, the target values outside of the [0,100] were fixed\n",
    "    . Moreover, the rows with indices 11041 and 11042 are no longer in the datarame. This is because the corresponding rows have the same inchikeys, but with values that\n",
    "    are more than 20 % apart. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atom_list = dd2_data_df['largest_frag'].apply(lambda mol: list(set([atom.GetAtomicNum() for atom in mol.GetAtoms()])))\n",
    "# atom_list = list(set(utilities.flatten_list(atom_list.values.tolist())))\n",
    "\n",
    "# print(\"List of unique atomic numbers: \",  [f\"{i}:{Chem.Atom(i).GetSymbol()}\" for i in atom_list])\n",
    "\n",
    "atom_list = [35, 6, 7, 8, 9, 15, 16, 17, 53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: cursive; font-size=16px\">Instead of defining a long range of values for possible atomic numbers (e.g.: 1-100), which could lead to a very sparse matrix, we can instead focus on the list of atoms that commonly occur in drug-like compounds. Specifically in this set, we notice that only 9 atoms occur in the list of tested compounds. If we add Hydrogen (which will be expicit), we have a vector of length 10 to represent the atomic number, plus 1 additional bit for unknown atomic numbers.</p>\n",
    "<p style=\"font-family: cursive; font-size=16px\">Le'ts define the atom anf bond featurizers.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = featurizers.AtomFeaturizer(\n",
    "    allowable_sets_one_hot={\n",
    "        \"atomic_num\": set(atom_list + [1]),\n",
    "        \"n_valence\": {0, 1, 2, 3, 4, 5, 6},\n",
    "        \"n_hydrogens\": {0, 1, 2, 3, 4},\n",
    "        \"hybridization\": {\"s\", \"sp\", \"sp2\", \"sp3\", \"sp3\", \"sp3d\", \"sp3d2\"},\n",
    "        \"chiral_tag\": {0,1,2,3},\n",
    "        \"is_aromatic\": {True, False},\n",
    "        \"is_in_ring\": {True, False} ,\n",
    "        \"is_in_ring_size_4\": {True, False},\n",
    "        \"is_in_ring_size_5\": {True, False},\n",
    "        \"is_in_ring_size_6\": {True, False}\n",
    "    }\n",
    "    , continuous_props = ['atomic_mass', 'atomic_vdw_radius', 'atomic_covalent_radius']\n",
    ")\n",
    "\n",
    "bond_featurizer = featurizers.BondFeaturizer(\n",
    "    allowable_sets_one_hot={\n",
    "        \"bond_type\": {\"single\", \"double\", \"triple\", \"aromatic\"},\n",
    "        \"conjugated\": {True, False},\n",
    "        \"stereo\": {\"stereonone, stereoz, stereoe, stereocis, stereotrans\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('atom_featurizer = ', atom_featurizer.features_mapping) \n",
    "# print('\\nbond_featurizer = ', bond_featurizer.features_mapping) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family: cursive\"> Data splitting</h4>\n",
    "<p style=\"font-family: cursive; font-size=16px\">We will split the data into train, validation and test sets, using the scaffold splitting method. Small-molecule scaffold splitting, particularly using the Bemis-Murcko scaffold approach, involves identifying and extracting the core structural framework shared among a set of chemical compounds, excluding variable substituents. This process simplifies and abstracts the diverse chemical structures, aiding in the analysis of chemical libraries and structure-activity relationships. The Bemis-Murcko scaffold represents the most conserved structural elements of the compounds.<br/>While scaffold splitting facilitates the identification of key structural motifs,and can enhance model interpretation, it has limitations in handling highly flexible or structurally diverse molecules,and may result in the loss of detailed iformation about specific functional groups or substituents, potentially oversimplifying structure-activity relationships. Moreover, In some cases, scaffold splitting may lead to overly strict splitting criteria, resulting in training, validation, and test sets that are too similar in chemical space.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molecules = dd2_data_df['largest_frag'].values\n",
    "# targets = dd2_data_df[target_column].values\n",
    "\n",
    "# splitter= splitters.ScaffoldSplitter()\n",
    "# train_idx, val_idx, test_idx = splitter.train_val_test_split(molecules=molecules[:], train_ratio=0.9, val_ratio=0.05\n",
    "#                             , test_ratio=0.05, return_as_indices=True, return_as_clusters=False, include_chirality=False\n",
    "#                             , sort_by_size=True, shuffle_idx=True, random_state=seed)\n",
    "\n",
    "# # csplitter = splitters.ClusterSplitter()\n",
    "# # train_idx, val_idx, test_idx = csplitter.train_val_test_split(molecules=molecules[:], train_ratio=0.8, val_ratio=0.1\n",
    "# #                             , test_ratio=0.1, return_as_indices=True, return_as_clusters=False, include_chirality=True\n",
    "# #                             , sort_by_size=False, shuffle_idx=False, random_state=seed, sim_cutoff=0.5)\n",
    "\n",
    "# # kfold_splits = splitter.kfold_split(molecules=molecules[:50], n_folds=3, return_as_indices=False, include_chirality=False\n",
    "# #                             , random_state=1)\n",
    "# # print(f\"fold sizes: {[len(f) for f in kfold_splits]}\")\n",
    "\n",
    "# train_mols, y_train = [molecules[i] for i in train_idx], targets[train_idx]\n",
    "# val_mols, y_val     = [molecules[i] for i in val_idx], targets[val_idx]\n",
    "# test_mols, y_test   = [molecules[i] for i in test_idx], targets[test_idx]\n",
    "\n",
    "# print(f\"Dataset sizes: Train ({len(train_mols)}) - Validation ({len(val_mols)}) - Test ({len(test_mols)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw.MolsToGridImage(sample(train_mols, 5) +sample(val_mols, 5) + sample(test_mols, 5)\n",
    "#                      , molsPerRow=5, subImgSize=(200, 150)\n",
    "#                      , legends=['train'] * 5 + ['val'] * 5 + ['test'] * 5, highlightAtomLists=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75 µs, sys: 50 µs, total: 125 µs\n",
      "Wall time: 123 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# features_ = None\n",
    "features_ = [\n",
    "                'CalcExactMolWt'\n",
    "                , 'CalcTPSA', 'CalcNumAromaticRings', 'CalcNumHBA', 'CalcNumHBD'\n",
    "                , 'CalcNumRotatableBonds'\n",
    "                # , 'CalcChi0n', 'CalcChi0v', 'CalcChi1n', 'CalcChi1v', 'CalcChi2n', 'CalcChi2v'\n",
    "                # , 'CalcChi3n', 'CalcChi3v', 'CalcChi4n', 'CalcChi4v', 'CalcChi0n', 'CalcChi0v'\n",
    "                , 'MolLogP', 'HallKierAlpha', 'qed', 'MaxPartialCharge', 'MinPartialCharge'\n",
    "                # , 'ZagrebIndex'\n",
    "                , 'MoeType'\n",
    "            ]\n",
    "\n",
    "mol_featurizer = featurizers.MoleculeFeaturizer(features = features_, df_func_gps=featurizers.DF_FUNC_GRPS_MINI) #featurizers.DF_FUNC_GRPS\n",
    "\n",
    "# train_dataset  = gnn_utils.get_dataset_from_mol_list(mol_list=train_mols[:], targets=y_train[:]\n",
    "#                                                     , atom_featurizer=atom_featurizer, bond_featurizer=bond_featurizer\n",
    "#                                                     , mol_featurizer=mol_featurizer\n",
    "#                                                     , add_explicit_h=True, compute_global_features=True\n",
    "#                                                     , add_global_feat_to_nodes=False)\n",
    "\n",
    "# print(train_dataset[1].x, train_dataset[1].y, train_dataset[1].x[0].shape) \n",
    "# if 'global_feats' in train_dataset[1].to_dict():\n",
    "#     print(train_dataset[1].global_feats.shape)\n",
    "# # train_dataset[1].x[5:10]\n",
    "\n",
    "# # s = mol_featurizer.compute_rdkit_properties(train_mols[0])\n",
    "# s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = gnn_utils.get_dataset_from_mol_list(mol_list=val_mols, targets=y_val\n",
    "#                                                     , atom_featurizer=atom_featurizer, bond_featurizer=bond_featurizer\n",
    "#                                                     , mol_featurizer=mol_featurizer\n",
    "#                                                     , add_explicit_h=True, compute_global_features=True\n",
    "#                                                     , add_global_feat_to_nodes=False)\n",
    "\n",
    "# test_dataset = gnn_utils.get_dataset_from_mol_list(mol_list=test_mols, targets=y_test\n",
    "#                                                     , atom_featurizer=atom_featurizer, bond_featurizer=bond_featurizer\n",
    "#                                                     , mol_featurizer=mol_featurizer\n",
    "#                                                     , add_explicit_h=True, compute_global_features=True\n",
    "#                                                     , add_global_feat_to_nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd2_dataset_fname = f'{DATASET_DIR}/dd2_datasets_objects_.pt'\n",
    "# torch.save({\n",
    "#             \"splitter\": splitter.__class__.__name__\n",
    "#             , \"train\": train_dataset\n",
    "#             , \"val\": val_dataset\n",
    "#             , \"test\": test_dataset\n",
    "#         }          \n",
    "#         , dd2_dataset_fname\n",
    "# )\n",
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd2_dataset_fname = f'{DATASET_DIR}/dd2_datasets.pt'\n",
    "dd2_dataset = torch.load(dd2_dataset_fname)  \n",
    "\n",
    "train_dataset = dd2_dataset['train_dataset']\n",
    "val_dataset   = dd2_dataset['val_dataset']\n",
    "test_dataset  = dd2_dataset['test_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfeatures_cleaned (10816, 104)\n",
      "gfeatures_cleaned (676, 104)\n",
      "gfeatures_cleaned (2028, 104)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "batch_size=128\n",
    "train_loader, val_loader, test_loader =datasets.get_dataloaders(train_data=train_dataset, test_data=test_dataset\n",
    "                                                                 , val_data=val_dataset, batch_size=batch_size, shuffle_train=True\n",
    "                                                                 , add_global_feats_to_nodes=True, num_workers=0\n",
    "                                                                 , scale_features=True, feature_scaler=StandardScaler())\n",
    "\n",
    "\n",
    "dd2_loader_fname = f'{DATASET_DIR}/dd2_dataloaders_for_gin.pt'\n",
    "torch.save({'train_loader':train_loader, 'val_loader':val_loader, 'test_loader':test_loader}, dd2_loader_fname)    \n",
    "# list(train_loader)[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd2_loader_fname = f'{DATASET_DIR}/dd2_dataloaders_for_gins.pt'\n",
    "# dd2_loaders = torch.load(dd2_loader_fname)  \n",
    "\n",
    "# train_loader = dd2_loaders['train_loader']\n",
    "# val_loader   = dd2_loaders['val_loader']\n",
    "# test_loader  = dd2_loaders['test_loader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: cursive\">Implementing a Graph Isomorphism Network model</h3>\n",
    "<p style=\"font-family: cursive\">\n",
    "A Graph Isomorphism Network (GIN) is a type of Graph Neural Network (GNN) that aggregates information from neighboring nodes and edges to learn node representations. GIN differs from other GNNs like Graph Attention Networks (GATs) and Graph Convolutional Networks (GCNs) by employing a readout function that integrates global graph-level information, allowing GIN to be more permutation invariant and capture graph isomorphism. GIN has been shown to be expressive and flexible, making it suitable for tasks where capturing structural similarities in graphs is crucial.</p>\n",
    "<ol>\n",
    "<li><a href=\"https://arxiv.org/abs/1810.00826\">How Powerful are Graph Neural Networks?</li>\n",
    "<li><a href=\"https://github.com/weihua916/powerful-gnns\">How Powerful are Graph Neural Networks? (GitHub)</li>\n",
    "<li><a href=\"https://medium.com/@mymomo119966.mm/graph-neural-networks-regressive-modelling-by-example-b13055bf56e8\">Graph Isomorphism Neural Networks — Drug Discovery Example</li>\n",
    "<li><a href=\"https://towardsdatascience.com/how-to-design-the-most-powerful-graph-neural-network-3d18b07a6e66\">GIN: How to Design the Most Powerful Graph Neural Network</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: cursive\">Creating and Training a model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 17:28:03,138] A new study created in memory with name: my_gin_study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params = {'task': 'binary_classification', 'in_channels': 159, 'global_fdim': None, 'model': 'graph_nns.GIN', 'gnn_hidden_neurons': 328, 'gnn_nlayers': 6, 'ffn_hidden_neurons': 260, 'ffn_nlayers': 2, 'out_neurons': 1, 'dropout_rate': 0.5991433714786851, 'activation_func': 'torch.nn.ReLU', 'gpooling_func': 'mean', 'learning_rate': 0.04235075675058081, 'n_epochs': 400, 'criterion': BCELoss(), 'optimizer': \"{'optimizer_type':'torch.optim.SGD', 'weight_decay':1e-3}\", 'scheduler': \"{'lr_scheduler_type':'lr_scheduler.CyclicLR', 'max_lr':0.2, 'step_size_up':75, 'cycle_momentum':True}\", 'scoring_func': 'roc_auc_score', 'add_batch_norm': True}\n",
      "in_channels = 159 :: gnn_dim = 328 :: activation_func = ReLU()\n",
      "\toptimizer:  {'state': {}, 'param_groups': [{'lr': 0.04235075675058081, 'momentum': 0, 'dampening': 0, 'weight_decay': 0.001, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]}\n",
      "\tlr_scheduler:  {'max_lrs': [0.2], 'total_size': 150.0, 'step_ratio': 0.5, 'mode': 'triangular', 'gamma': 1.0, '_scale_fn_custom': None, 'scale_mode': 'cycle', 'cycle_momentum': True, 'base_momentums': [0.8], 'max_momentums': [0.9], 'base_lrs': [0.04235075675058081], 'last_epoch': 0, 'verbose': False, '_step_count': 1, '_get_lr_called_within_step': False, '_last_lr': [0.04235075675058081]}\n",
      "checkpoint dir: None\n",
      "device = cuda\n",
      "TASK: binary_classification\n",
      "\n",
      "===> Epoch    1/400: Average Train Loss: 2.830 |  Average validation Loss: 2.892 | Validation Score: 0.661 | lr: 0.04445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-22 17:28:44,590] Trial 0 failed with parameters: {'in_channels': 159, 'model': 'graph_nns.GIN', 'gnn_hidden_neurons': 328, 'gnn_nlayers': 6, 'ffn_hidden_neurons': 260, 'ffn_nlayers': 2, 'out_neurons': 1, 'dropout_rate': 0.5991433714786851, 'activation_func': 'torch.nn.ReLU', 'gpooling_func': 'mean', 'learning_rate': 0.04235075675058081, 'n_epochs': 400, 'optimizer': \"{'optimizer_type':'torch.optim.SGD', 'weight_decay':1e-3}\", 'scheduler': \"{'lr_scheduler_type':'lr_scheduler.CyclicLR', 'max_lr':0.2, 'step_size_up':75, 'cycle_momentum':True}\", 'scoring_func': 'roc_auc_score'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djoy2409-wsl/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/lib/training_utils.py\", line 679, in <lambda>\n",
      "    objective = lambda trial: self.objective(\n",
      "  File \"/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/lib/training_utils.py\", line 649, in objective\n",
      "    val_score = self.train_and_validate(\n",
      "  File \"/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/lib/training_utils.py\", line 569, in train_and_validate\n",
      "    train_losses, val_losses, val_scores = gnn_trainer.train(\n",
      "  File \"/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/lib/training_utils.py\", line 254, in train\n",
      "    train_loss = self.train_epoch(self.model, train_loader, device)\n",
      "  File \"/home/djoy2409-wsl/projects/software_development/qsar_w_gnns/lib/training_utils.py\", line 327, in train_epoch\n",
      "    loss.backward()\n",
      "  File \"/home/djoy2409-wsl/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/djoy2409-wsl/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-22 17:28:44,595] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:32\u001b[0m\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:690\u001b[0m, in \u001b[0;36mOptunaHPO.run_optimization\u001b[0;34m(self, train_val_data, params_grid, optuna_direction, split_mode, study_name, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m    676\u001b[0m     direction\u001b[38;5;241m=\u001b[39moptuna_direction, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler, study_name\u001b[38;5;241m=\u001b[39mstudy_name\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    679\u001b[0m objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective(\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;66;03m# task=task,\u001b[39;00m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;66;03m# gnn_model=gnn_model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    688\u001b[0m )\n\u001b[0;32m--> 690\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params,\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# , \"gnn_type\": gnn_model.__class__.__name__\u001b[39;00m\n\u001b[1;32m    698\u001b[0m }\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:679\u001b[0m, in \u001b[0;36mOptunaHPO.run_optimization.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_optimization\u001b[39m(\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;66;03m# gnn_model:torch.nn.Module,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    674\u001b[0m ):\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m    676\u001b[0m         direction\u001b[38;5;241m=\u001b[39moptuna_direction, sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler, study_name\u001b[38;5;241m=\u001b[39mstudy_name\n\u001b[1;32m    677\u001b[0m     )\n\u001b[0;32m--> 679\u001b[0m     objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# task=task,\u001b[39;49;00m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# gnn_model=gnn_model,\u001b[39;49;00m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_val_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_val_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trainer_params_grid=trainer_params_grid,\u001b[39;49;00m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m    691\u001b[0m         objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_trials, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, gc_after_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_params,\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_value\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# , \"gnn_type\": gnn_model.__class__.__name__\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     }\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:649\u001b[0m, in \u001b[0;36mOptunaHPO.objective\u001b[0;34m(self, trial, train_val_data, params_grid, split_mode, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# print(\"n_epochs\", params['n_epochs'])\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# print(\"model\", params['model'])\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# print('gnn_trainer', gnn_trainer.__dict__)\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# print('type', type(dataloaders[0]))\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataloaders[\u001b[38;5;241m0\u001b[39m], DataLoader):\n\u001b[0;32m--> 649\u001b[0m     val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# gnn_trainer = gnn_trainer,\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# print('val_score', val_score)\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val_score\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:569\u001b[0m, in \u001b[0;36mOptunaHPO.train_and_validate\u001b[0;34m(self, params, dataloaders, split_mode)\u001b[0m\n\u001b[1;32m    566\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m gnn_model\n\u001b[1;32m    567\u001b[0m     gnn_trainer \u001b[38;5;241m=\u001b[39m GNNTrainer\u001b[38;5;241m.\u001b[39mfrom_dict(params)\n\u001b[0;32m--> 569\u001b[0m     train_losses, val_losses, val_scores \u001b[38;5;241m=\u001b[39m \u001b[43mgnn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;66;03m# return val_losses[-1]\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:254\u001b[0m, in \u001b[0;36mGNNTrainer.train\u001b[0;34m(self, train_loader, val_loader, save_every, n_epochs, device)\u001b[0m\n\u001b[1;32m    252\u001b[0m     n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m--> 254\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m    257\u001b[0m     val_loss, val_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_epoch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, val_loader, device)\n",
      "File \u001b[0;32m~/projects/software_development/qsar_w_gnns/lib/training_utils.py:327\u001b[0m, in \u001b[0;36mGNNTrainer.train_epoch\u001b[0;34m(self, model, train_loader, device)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# print(f\"train_true = {train_true.view(-1,1).shape}\")\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# print(f\"Criterion: {self.criterion}\")\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# print(f\"Train Loss = {loss}\")\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was an issue computing the loss for training batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_nr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. The loss was set to None, and will not be considered for averaging.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chemai-gpu/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sklearn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "in_channels = list(train_loader)[0].x[0].shape[0]\n",
    "global_fdim = list(train_loader)[0].global_feats.shape[1] if 'global_feats' in list(train_loader)[0].to_dict() else None\n",
    "criterion   = BCELoss()\n",
    "\n",
    "exp_params_grid = {\n",
    "    'task': 'binary_classification','in_channels' : in_channels, 'global_fdim' : global_fdim,\n",
    "    'model': ['graph_nns.GIN'],\n",
    "    # 'gnn_hidden_neurons': [128, 256, 384, 512]\n",
    "    'gnn_hidden_neurons': [200, 400]\n",
    "    , 'gnn_nlayers':[4, 6]\n",
    "    # , 'ffn_hidden_neurons': [64, 128, 256, 384, 512]\n",
    "    , 'ffn_hidden_neurons': [100, 400]\n",
    "    , 'ffn_nlayers': [1,2], 'out_neurons':1, 'dropout_rate': [0.1, 0.6]\n",
    "    , 'activation_func': ['torch.nn.LeakyReLU', 'torch.nn.ReLU']\n",
    "    , 'gpooling_func' : ['mean'],\n",
    "\n",
    "    'learning_rate': [5e-2, 1e-2], 'n_epochs': [400, 700],\n",
    "    'criterion' : criterion,\n",
    "    'optimizer' : [\"{'optimizer_type':'torch.optim.SGD', 'weight_decay':1e-3}\"], \n",
    "    # 'scheduler' : [\"{'lr_scheduler_type':'lr_scheduler.ReduceLROnPlateau', 'mode':'min', 'factor':0.7, 'patience':30, 'threshold':5e-02, 'eps':1e-08}\"], \n",
    "    'scheduler' : [\"{'lr_scheduler_type':'lr_scheduler.CyclicLR', 'max_lr':0.2, 'step_size_up':75, 'cycle_momentum':True}\"], \n",
    "\n",
    "    'scoring_func': ['roc_auc_score'],\n",
    "    'add_batch_norm':True\n",
    "}\n",
    "\n",
    "my_hpo = training_utils.OptunaHPO(n_trials=3, n_jobs=1, sampler=None)\n",
    "\n",
    "results = my_hpo.run_optimization(   \n",
    "            train_val_data= [train_loader, val_loader]\n",
    "            , params_grid=exp_params_grid\n",
    "            , optuna_direction='maximize'\n",
    "            , split_mode='classic'\n",
    "            , study_name='my_gin_study'\n",
    "        )\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import inspect\n",
    "# import ast\n",
    "# d = json.loads('{\"a\":1, \"b\":2, \"c\":3}')\n",
    "# del d[\"b\"]\n",
    "# print(d)\n",
    "# # print(d = json.loads('{\"a\":1, \"b\":2, \"c\":3}'))\n",
    "# d = '{\"a\":1, \"b\":5, \"c\":3}'\n",
    "# print(ast.literal_eval(d))\n",
    "# print(\"{'a':1, 'b':5}\")\n",
    "\n",
    "# prms = list(inspect.signature(torch.optim.SGD).parameters.keys())\n",
    "# prms.remove('params')\n",
    "# prms\n",
    "\n",
    "# dir(torch.optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-family: cursive\">Training and Evaluating The Best Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = results['best_params']\n",
    "best_params.update(**{'task':'binary_classification'})\n",
    "print(\"Best params: \", best_params)\n",
    "\n",
    "gin_model = graph_nns.GIN.from_dict({\n",
    "    \"task\" : best_params['task']\n",
    "    , 'in_channels': best_params['in_channels'],  'gnn_hidden_neurons' : int(best_params['gnn_hidden_neurons'])\n",
    "    , 'gnn_nlayers' : best_params['gnn_nlayers'], 'global_fdim' : best_params.get('global_fdim', None)\n",
    "    , 'ffn_hidden_neurons' :int(best_params['ffn_hidden_neurons']), 'ffn_nlayers' : best_params['ffn_nlayers']\n",
    "    , 'out_neurons' : best_params['out_neurons'], 'dropout_rate' : best_params['dropout_rate']\n",
    "    , 'gpooling_func' : best_params['gpooling_func'], 'activation_func' : best_params['activation_func']\n",
    "    , 'add_batch_norm' : True\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family: cursive\"> Model Training</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_trainer = training_utils.GNNTrainer(task=best_params['task'], model=gcn_model\n",
    "                                            , scheduler=best_params['scheduler']\n",
    "                                            , optimizer=best_params['optimizer']\n",
    "                                            , criterion=criterion\n",
    "                                            , scoring_func=eval(best_params['scoring_func'])\n",
    "                                            , n_epochs=best_params['n_epochs']\n",
    "                                            , learning_rate=best_params['learning_rate']\n",
    "                                        )\n",
    "gnn_trainer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_losses, val_losses, val_scores = gnn_trainer.train(train_loader=train_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m utilities\u001b[38;5;241m.\u001b[39mplots_train_val_metrics(train_losses\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_losses\u001b[49m, val_scores\u001b[38;5;241m=\u001b[39mval_scores\n\u001b[1;32m      2\u001b[0m                             , val_losses\u001b[38;5;241m=\u001b[39mval_losses, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m      3\u001b[0m                             , image_pathname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASET_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/models/dd2_class_gnn_stats_gin.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m                             , val_score_name\u001b[38;5;241m=\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscoring_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "utilities.plots_train_val_metrics(train_losses=train_losses, val_scores=val_scores\n",
    "                            , val_losses=val_losses, figsize=(10, 7)\n",
    "                            , image_pathname=f\"{DATASET_DIR}/models/dd2_class_gnn_stats_gin.jpeg\"\n",
    "                            , val_score_name=best_params['scoring_func'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gin_model.state_dict(), f\"{DATASET_DIR}/models/dd2_class_gin_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"font-family: cursive\"> Model Evaluation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import predictions\n",
    "test_pred, test_true = predictions.predict_from_loader(loader=test_loader, model=gin_model\n",
    "                                                     , device='cuda:0'\n",
    "                                                     , return_true_targets=True, desc=\"Predicting...\")\n",
    "\n",
    "threshold = 0.5\n",
    "test_pred_classes = [int(x>threshold) for x in test_pred.squeeze(1)] \n",
    "\n",
    "# print(test_pred)\n",
    "print(test_pred_classes)\n",
    "\n",
    "print(f\"\\nROC AUC Score = {round(roc_auc_score(test_true.cpu(), test_pred.detach().cpu()),3)}\")\n",
    "print(f\"Balanced Acc. = {round(balanced_accuracy_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"F1-Score      = {round(f1_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"Precision     = {round(precision_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"Recall        = {round(recall_score(test_true.cpu(), test_pred_classes),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"font-family: cursive\"> Evaluate using the GNNPredictor</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_predictor = predictions.GNNPredictor(model=gin_model, atom_featurizer=atom_featurizer\n",
    "                                        , bond_featurizer=bond_featurizer, add_explicit_h=True\n",
    "                                        , mol_featurizer=mol_featurizer\n",
    "                                        , scale_features=True, feature_scaler=StandardScaler()\n",
    "                                        , compute_global_features=True, add_global_feats_to_nodes=True)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gin_model = torch.load(f\"{DATASET_DIR}/models/dd2_class_gin_model.pt\")\n",
    "torch.save(gin_predictor, f\"{DATASET_DIR}/models/dd2_class_gin_predictor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_predictor=torch.load(f\"{DATASET_DIR}/models/dd2_class_gin_predictor.pt\")\n",
    "gin_predictor\n",
    "\n",
    "dd2_loader_fname = f'{DATASET_DIR}/dd2_dataloaders.pt'\n",
    "dd2_loaders = torch.load(dd2_loader_fname)  \n",
    "\n",
    "train_loader = dd2_loaders['train_loader']\n",
    "val_loader   = dd2_loaders['val_loader']\n",
    "test_loader  = dd2_loaders['test_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred, test_true = gin_predictor.predict_from_loader(loader=test_loader\n",
    "                                                     , device='cuda:0', return_true_targets=True, desc=\"Predicting...\")\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "test_pred_classes = [int(x>threshold) for x in test_pred.squeeze(1)] \n",
    "print(test_pred_classes)\n",
    "\n",
    "print(f\"\\nROC AUC Score = {round(roc_auc_score(test_true.cpu(), test_pred.detach().cpu()),3)}\")\n",
    "print(f\"Balanced Acc. = {round(balanced_accuracy_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"F1-Score      = {round(f1_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"Precision     = {round(precision_score(test_true.cpu(), test_pred_classes),3)}\")\n",
    "print(f\"Recall        = {round(recall_score(test_true.cpu(), test_pred_classes),3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"font-family: cursive\"> Predict DD2 Activity from SMILES</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = ['CCC(=O)NCc1nccc1CO', 'O=C(O)c1cccnc1Sc1cc(CCO)c(Cl)cc1']\n",
    "new_preds = gin_predictor.predict_from_smiles_list(smiles_list=smiles, device='cuda:0'\n",
    "                                                , desc=\"Predicting...\")\n",
    "\n",
    "threshold = 0.5\n",
    "pred_classes = [int(x>threshold) for x in new_preds.squeeze(1)]\n",
    "print(\"\\nnew_preds\", new_preds)\n",
    "print(\"pred_classes\", pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemai-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
